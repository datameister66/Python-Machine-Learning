{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest, XGBoost, and Boruta Feature Selection in Python\n",
    "\n",
    "This notebook converts the Random Forest, XGBoost, and Boruta feature selection sections from an R script to Python. It processes the 'santander' dataset, trains a Random Forest model, performs feature selection, trains an XGBoost model, and applies Boruta feature selection. The code uses scikit-learn, xgboost, and BorutaPy libraries to replicate the R functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Install and import necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if running in a fresh environment\n",
    "!pip install pandas numpy scikit-learn xgboost borutapy matplotlib seaborn sklearn-metrics\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import xgboost as xgb\n",
    "from borutapy import BorutaPy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1966)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data\n",
    "\n",
    "Load the preprocessed 'santander' dataset and split into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (assuming santander_prepd.RData equivalent is a CSV or similar)\n",
    "# Replace with actual file path\n",
    "santander = pd.read_csv('santander_prepd.csv')  # Update path as needed\n",
    "\n",
    "# Display class distribution\n",
    "print(santander['y'].value_counts())\n",
    "\n",
    "# Convert target to categorical if needed\n",
    "santander['y'] = santander['y'].astype('category')\n",
    "\n",
    "# Split data into train and test\n",
    "train, test = train_test_split(santander, test_size=0.2, stratify=santander['y'], random_state=1966)\n",
    "\n",
    "# Remove near-zero variance features\n",
    "selector = VarianceThreshold(threshold=0)\n",
    "train_features = selector.fit_transform(train.drop('y', axis=1))\n",
    "selected_features = train.drop('y', axis=1).columns[selector.get_support()]\n",
    "train = pd.concat([train[selected_features], train['y']], axis=1)\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train.drop('y', axis=1)\n",
    "y_train = train['y']\n",
    "X_test = test[selected_features]\n",
    "y_test = test['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model\n",
    "\n",
    "Train a Random Forest model and evaluate feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for model\n",
    "np.random.seed(1999)\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, class_weight={0: 1, 1: 3}, random_state=1999)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'var': X_train.columns,\n",
    "    'MeanDecreaseGini': rf.feature_importances_\n",
    "})\n",
    "print(feature_importance.describe())\n",
    "\n",
    "# Select important features (threshold based on mean importance)\n",
    "threshold = feature_importance['MeanDecreaseGini'].mean()\n",
    "important_features = feature_importance[feature_importance['MeanDecreaseGini'] > threshold]['var']\n",
    "X_train_reduced = X_train[important_features]\n",
    "X_test_reduced = X_test[important_features]\n",
    "\n",
    "# Train reduced Random Forest\n",
    "np.random.seed(567)\n",
    "rf_reduced = RandomForestClassifier(n_estimators=110, class_weight={0: 1, 1: 3}, random_state=567)\n",
    "rf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Predictions and probabilities\n",
    "rf_prob = rf.predict_proba(X_train)[:, 1]\n",
    "y_num = y_train.astype(int)\n",
    "\n",
    "# Evaluate\n",
    "print('AUC:', roc_auc_score(y_num, rf_prob))\n",
    "print('Log Loss:', log_loss(y_num, rf_prob))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='MeanDecreaseGini', y='var', data=feature_importance.sort_values('MeanDecreaseGini', ascending=False))\n",
    "plt.title('Random Forest Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corrected Probability Function\n",
    "\n",
    "Define a function to correct class probabilities based on population and sample fractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrected_prob(result, population_fraction, sample_fraction):\n",
    "    value = 1 / (1 + (1 / population_fraction - 1) / (1 / sample_fraction - 1) * (1 / result - 1))\n",
    "    return value\n",
    "\n",
    "# Apply correction\n",
    "y_prob_corrected = corrected_prob(rf_prob, population_fraction=0.04, sample_fraction=0.33)\n",
    "\n",
    "# Evaluate corrected probabilities\n",
    "print('Corrected AUC:', roc_auc_score(y_num, y_prob_corrected))\n",
    "print('Corrected Log Loss:', log_loss(y_num, y_prob_corrected))\n",
    "\n",
    "# Test set predictions\n",
    "rf_test_prob = rf.predict_proba(X_test)[:, 1]\n",
    "rf_test_corrected = corrected_prob(rf_test_prob, population_fraction=0.04, sample_fraction=0.33)\n",
    "y_test_num = y_test.astype(int)\n",
    "\n",
    "print('Test AUC:', roc_auc_score(y_test_num, rf_test_corrected))\n",
    "print('Test Log Loss:', log_loss(y_test_num, rf_test_corrected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Model\n",
    "\n",
    "Train an XGBoost model with hyperparameter tuning and evaluate performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'colsample_bytree': [1],\n",
    "    'min_child_weight': [1],\n",
    "    'learning_rate': [0.1, 0.3, 0.5],\n",
    "    'gamma': [0.25, 0.5],\n",
    "    'subsample': [1],\n",
    "    'max_depth': [3]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost\n",
    "xgb_model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='error', random_state=123)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='roc_auc', verbose=2)\n",
    "grid_search.fit(X_train_reduced, y_train)\n",
    "\n",
    "print('Best parameters:', grid_search.best_params_)\n",
    "\n",
    "# Train final model\n",
    "xgb_final = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='error',\n",
    "    learning_rate=0.5,\n",
    "    max_depth=3,\n",
    "    subsample=1,\n",
    "    colsample_bytree=1,\n",
    "    gamma=0.25,\n",
    "    n_estimators=100,\n",
    "    random_state=1232\n",
    ")\n",
    "xgb_final.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Feature importance\n",
    "imp_matrix = pd.DataFrame({\n",
    "    'Feature': X_train_reduced.columns,\n",
    "    'Gain': xgb_final.feature_importances_\n",
    "})\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Gain', y='Feature', data=imp_matrix.sort_values('Gain', ascending=False))\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# Predictions and evaluation\n",
    "xgb_pred = xgb_final.predict_proba(X_train_reduced)[:, 1]\n",
    "print('Train AUC:', roc_auc_score(y_num, xgb_pred))\n",
    "print('Train Log Loss:', log_loss(y_num, xgb_pred))\n",
    "\n",
    "# Test set predictions\n",
    "xgb_test_pred = xgb_final.predict_proba(X_test_reduced)[:, 1]\n",
    "print('Test AUC:', roc_auc_score(y_test_num, xgb_test_pred))\n",
    "print('Test Log Loss:', log_loss(y_test_num, xgb_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve Comparison\n",
    "\n",
    "Compare ROC curves for Random Forest and XGBoost models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test_num, rf_test_corrected)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test_num, xgb_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_rf, tpr_rf, label='Random Forest', color='black')\n",
    "plt.plot(fpr_xgb, tpr_xgb, label='XGBoost', color='green')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boruta Feature Selection\n",
    "\n",
    "Apply Boruta feature selection and train a Random Forest on selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sim_df for Boruta (replace with actual file path)\n",
    "sim_df = pd.read_csv('sim_df.csv')  # Update path as needed\n",
    "sim_df['y'] = sim_df['y'].astype('category')\n",
    "\n",
    "# Split data\n",
    "train, test = train_test_split(sim_df, test_size=0.3, stratify=sim_df['y'], random_state=1066)\n",
    "X_train = train.drop('y', axis=1)\n",
    "y_train = train['y']\n",
    "X_test = test.drop('y', axis=1)\n",
    "y_test = test['y']\n",
    "\n",
    "# Boruta feature selection\n",
    "np.random.seed(5150)\n",
    "rf_boruta = RandomForestClassifier(n_jobs=-1, random_state=5150)\n",
    "boruta = BorutaPy(rf_boruta, n_estimators='auto', random_state=5150)\n",
    "boruta.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_train.columns[boruta.support_]\n",
    "print('Selected features:', selected_features)\n",
    "\n",
    "# Train Random Forest on selected features\n",
    "np.random.seed(999)\n",
    "boruta_rf = RandomForestClassifier(random_state=999)\n",
    "boruta_rf.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "boruta_pred = boruta_rf.predict_proba(X_test[selected_features])[:, 1]\n",
    "y_test_num = y_test.astype(int)\n",
    "print('Boruta RF AUC:', roc_auc_score(y_test_num, boruta_pred))\n",
    "print('Boruta RF Log Loss:', log_loss(y_test_num, boruta_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}